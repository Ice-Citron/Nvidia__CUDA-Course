{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab45352e-9984-4731-ac23-58d30e42896d",
   "metadata": {},
   "source": [
    "```\n",
    "#include <stdio.h>\n",
    "\n",
    "#define N  64\n",
    "\n",
    "__global__ void matrixMulGPU( int * a, int * b, int * c )\n",
    "{\n",
    "  /*\n",
    "   * Build out this kernel.\n",
    "   */\n",
    "}\n",
    "\n",
    "/*\n",
    " * This CPU function already works, and will run to create a solution matrix\n",
    " * against which to verify your work building out the matrixMulGPU kernel.\n",
    " */\n",
    "\n",
    "void matrixMulCPU( int * a, int * b, int * c )\n",
    "{\n",
    "  int val = 0;\n",
    "\n",
    "  for( int row = 0; row < N; ++row )\n",
    "    for( int col = 0; col < N; ++col )\n",
    "    {\n",
    "      val = 0;\n",
    "      for ( int k = 0; k < N; ++k )\n",
    "        val += a[row * N + k] * b[k * N + col];\n",
    "      c[row * N + col] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  int *a, *b, *c_cpu, *c_gpu; // Allocate a solution matrix for both the CPU and the GPU operations\n",
    "\n",
    "  int size = N * N * sizeof (int); // Number of bytes of an N x N matrix\n",
    "\n",
    "  // Allocate memory\n",
    "  cudaMallocManaged (&a, size);\n",
    "  cudaMallocManaged (&b, size);\n",
    "  cudaMallocManaged (&c_cpu, size);\n",
    "  cudaMallocManaged (&c_gpu, size);\n",
    "\n",
    "  // Initialize memory; create 2D matrices\n",
    "  for( int row = 0; row < N; ++row )\n",
    "    for( int col = 0; col < N; ++col )\n",
    "    {\n",
    "      a[row*N + col] = row;\n",
    "      b[row*N + col] = col+2;\n",
    "      c_cpu[row*N + col] = 0;\n",
    "      c_gpu[row*N + col] = 0;\n",
    "    }\n",
    "\n",
    "  /*\n",
    "   * Assign `threads_per_block` and `number_of_blocks` 2D values\n",
    "   * that can be used in matrixMulGPU above.\n",
    "   */\n",
    "\n",
    "  dim3 threads_per_block;\n",
    "  dim3 number_of_blocks;\n",
    "\n",
    "  matrixMulGPU <<< number_of_blocks, threads_per_block >>> ( a, b, c_gpu );\n",
    "\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  // Call the CPU version to check our work\n",
    "  matrixMulCPU( a, b, c_cpu );\n",
    "\n",
    "  // Compare the two answers to make sure they are equal\n",
    "  bool error = false;\n",
    "  for( int row = 0; row < N && !error; ++row )\n",
    "    for( int col = 0; col < N && !error; ++col )\n",
    "      if (c_cpu[row * N + col] != c_gpu[row * N + col])\n",
    "      {\n",
    "        printf(\"FOUND ERROR at c[%d][%d]\\n\", row, col);\n",
    "        error = true;\n",
    "        break;\n",
    "      }\n",
    "  if (!error)\n",
    "    printf(\"Success!\\n\");\n",
    "\n",
    "  // Free all our allocated memory\n",
    "  cudaFree(a); cudaFree(b);\n",
    "  cudaFree( c_cpu ); cudaFree( c_gpu );\n",
    "}\n",
    "\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "#define N  64\n",
    "\n",
    "__global__ void matrixMulGPU( int * a, int * b, int * c )\n",
    "{\n",
    "  int val = 0;\n",
    "\n",
    "  int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "  if (row < N && col < N)\n",
    "  {\n",
    "    for ( int k = 0; k < N; ++k )\n",
    "      val += a[row * N + k] * b[k * N + col];\n",
    "    c[row * N + col] = val;\n",
    "  }\n",
    "}\n",
    "\n",
    "void matrixMulCPU( int * a, int * b, int * c )\n",
    "{\n",
    "  int val = 0;\n",
    "\n",
    "  for( int row = 0; row < N; ++row )\n",
    "    for( int col = 0; col < N; ++col )\n",
    "    {\n",
    "      val = 0;\n",
    "      for ( int k = 0; k < N; ++k )\n",
    "        val += a[row * N + k] * b[k * N + col];\n",
    "      c[row * N + col] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  int *a, *b, *c_cpu, *c_gpu;\n",
    "\n",
    "  int size = N * N * sizeof (int); // Number of bytes of an N x N matrix\n",
    "\n",
    "  // Allocate memory\n",
    "  cudaMallocManaged (&a, size);\n",
    "  cudaMallocManaged (&b, size);\n",
    "  cudaMallocManaged (&c_cpu, size);\n",
    "  cudaMallocManaged (&c_gpu, size);\n",
    "\n",
    "  // Initialize memory\n",
    "  for( int row = 0; row < N; ++row )\n",
    "    for( int col = 0; col < N; ++col )\n",
    "    {\n",
    "      a[row*N + col] = row;\n",
    "      b[row*N + col] = col+2;\n",
    "      c_cpu[row*N + col] = 0;\n",
    "      c_gpu[row*N + col] = 0;\n",
    "    }\n",
    "\n",
    "  dim3 threads_per_block (16, 16, 1); // A 16 x 16 block threads\n",
    "  dim3 number_of_blocks ((N / threads_per_block.x) + 1, (N / threads_per_block.y) + 1, 1);\n",
    "\n",
    "  matrixMulGPU <<< number_of_blocks, threads_per_block >>> ( a, b, c_gpu );\n",
    "\n",
    "  cudaDeviceSynchronize(); // Wait for the GPU to finish before proceeding\n",
    "\n",
    "  // Call the CPU version to check our work\n",
    "  matrixMulCPU( a, b, c_cpu );\n",
    "\n",
    "  // Compare the two answers to make sure they are equal\n",
    "  bool error = false;\n",
    "  for( int row = 0; row < N && !error; ++row )\n",
    "    for( int col = 0; col < N && !error; ++col )\n",
    "      if (c_cpu[row * N + col] != c_gpu[row * N + col])\n",
    "      {\n",
    "        printf(\"FOUND ERROR at c[%d][%d]\\n\", row, col);\n",
    "        error = true;\n",
    "        break;\n",
    "      }\n",
    "  if (!error)\n",
    "    printf(\"Success!\\n\");\n",
    "\n",
    "  // Free all our allocated memory\n",
    "  cudaFree(a); cudaFree(b);\n",
    "  cudaFree( c_cpu ); cudaFree( c_gpu );\n",
    "}\n",
    "```\n",
    "---\n",
    "\n",
    "The code you've shared shows a basic example of matrix multiplication using both CPU and GPU implementations to compare their correctness. Let's go through the key components of the GPU implementation, which uses NVIDIA CUDA to perform matrix multiplication in parallel:\n",
    "\n",
    "### CUDA Kernel for Matrix Multiplication\n",
    "The `matrixMulGPU` kernel function performs the matrix multiplication on the GPU. Each thread computes one element of the result matrix `c`. The indices for the row and column are determined using the thread and block dimensions and indices:\n",
    "- `int row = blockIdx.x * blockDim.x + threadIdx.x;`\n",
    "- `int col = blockIdx.y * blockDim.y + threadIdx.y;`\n",
    "\n",
    "If the computed row and column indices are within the bounds of the matrix (which is `N x N`), the kernel calculates the value for `c[row * N + col]` by performing the dot product of the respective row from matrix `a` and the column from matrix `b`:\n",
    "```c\n",
    "if (row < N && col < N)\n",
    "{\n",
    "  for (int k = 0; k < N; ++k)\n",
    "    val += a[row * N + k] * b[k * N + col];\n",
    "  c[row * N + col] = val;\n",
    "}\n",
    "```\n",
    "\n",
    "### Configuration of Blocks and Threads\n",
    "In the `main` function, the blocks and threads are configured to launch the kernel:\n",
    "```c\n",
    "dim3 threads_per_block (16, 16, 1); // A 16 x 16 block of threads\n",
    "dim3 number_of_blocks ((N / threads_per_block.x) + 1, (N / threads_per_block.y) + 1, 1);\n",
    "```\n",
    "This configuration defines each block to have `16x16` threads and calculates the number of blocks needed to cover all elements of the matrix. The `+1` ensures coverage in case `N` is not an exact multiple of the thread block size.\n",
    "\n",
    "### Memory Management\n",
    "Unified Memory is used for arrays `a`, `b`, `c_cpu`, and `c_gpu`:\n",
    "```c\n",
    "cudaMallocManaged (&a, size);\n",
    "cudaMallocManaged (&b, size);\n",
    "cudaMallocManaged (&c_cpu, size);\n",
    "cudaMallocManaged (&c_gpu, size);\n",
    "```\n",
    "This simplifies memory handling, allowing these arrays to be accessed from both the CPU and GPU without manual data transfers.\n",
    "\n",
    "### Synchronization and Verification\n",
    "After launching the kernel, `cudaDeviceSynchronize()` is called to ensure that all GPU operations are complete before the CPU accesses the GPU results. The results of the GPU computation (`c_gpu`) are then compared to those computed by the CPU (`c_cpu`) to check for correctness:\n",
    "```c\n",
    "for(int row = 0; row < N && !error; ++row)\n",
    "  for(int col = 0; col < N && !error; ++col)\n",
    "    if (c_cpu[row * N + col] != c_gpu[row * N + col])\n",
    "    {\n",
    "      printf(\"FOUND ERROR at c[%d][%d]\\n\", row, col);\n",
    "      error = true;\n",
    "      break;\n",
    "    }\n",
    "```\n",
    "\n",
    "### Cleanup\n",
    "Finally, the memory allocated for the arrays is freed using `cudaFree`.\n",
    "\n",
    "This setup illustrates how CUDA can be used for parallel computations on matrices, taking advantage of the GPU's architecture to perform operations in parallel across a large number of threads, significantly speeding up computations that are inherently parallelizable, such as matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7bb6ff-c2a5-4941-a4bf-e7075846c839",
   "metadata": {},
   "source": [
    "i dont get what the loop is making, in terms of why =col+2 and =row\n",
    "\n",
    "further, i also dont get whats dim3, and what are the last 2 lines doing\n",
    "\n",
    "\n",
    "```\n",
    "// Initialize memory\n",
    "  for( int row = 0; row < N; ++row )\n",
    "    for( int col = 0; col < N; ++col )\n",
    "    {\n",
    "      a[row*N + col] = row;\n",
    "      b[row*N + col] = col+2;\n",
    "      c_cpu[row*N + col] = 0;\n",
    "      c_gpu[row*N + col] = 0;\n",
    "    }\n",
    "\n",
    "  dim3 threads_per_block (16, 16, 1); // A 16 x 16 block threads\n",
    "  dim3 number_of_blocks ((N / threads_per_block.x) + 1, (N / threads_per_block.y) + 1, 1);\n",
    "```\n",
    "---\n",
    "\n",
    "Let's break down your questions to clarify the initialization of the matrices and the role of `dim3` in CUDA programming:\n",
    "\n",
    "### Matrix Initialization\n",
    "The initialization loop for matrices `a` and `b` is designed to fill these matrices with specific values that are simple and predictable, making it easy to verify the correctness of the matrix multiplication:\n",
    "\n",
    "```c\n",
    "for(int row = 0; row < N; ++row)\n",
    "  for(int col = 0; col < N; ++col)\n",
    "  {\n",
    "    a[row * N + col] = row;     // Each element in a row of matrix 'a' is set to the row index\n",
    "    b[row * N + col] = col + 2; // Each element in a row of matrix 'b' is set to the column index + 2\n",
    "    c_cpu[row * N + col] = 0;\n",
    "    c_gpu[row * N + col] = 0;\n",
    "  }\n",
    "```\n",
    "\n",
    "- `a[row*N + col] = row;` — Each element of matrix `a` is set to its row index. This means that all elements in the first row will be 0, in the second row 1, and so forth up to N-1. This creates a matrix where each row has a constant value equal to the row index.\n",
    "- `b[row*N + col] = col + 2;` — Each element of matrix `b` is set to its column index plus 2. This ensures that all elements in the first column are 2, in the second column are 3, and so forth. This setup makes each column's values increase linearly starting from 2.\n",
    "\n",
    "This initialization scheme helps in straightforward verification post-multiplication since the patterns are predictable and calculations can be easily traced.\n",
    "\n",
    "### Understanding `dim3`\n",
    "`dim3` is a type provided by CUDA to define a three-dimensional grid of blocks or threads. When launching a kernel, these dimensions dictate how many blocks and threads per block are used:\n",
    "\n",
    "- **threads_per_block** — Specifies the number of threads in each block. Here it is set to `16x16x1`, meaning each block contains 256 threads arranged in a 16 by 16 grid. The third dimension is generally used for more complex applications involving three-dimensional data.\n",
    "  \n",
    "- **number_of_blocks** — Determines how many blocks are used in the grid. The computation `(N / threads_per_block.x) + 1` ensures that there are enough blocks to cover all elements of the matrix, even if `N` isn't perfectly divisible by 16. This \"+1\" acts as a safety to cover all elements:\n",
    "  - `(N / threads_per_block.x) + 1` calculates how many blocks are needed along the x-dimension (rows).\n",
    "  - `(N / threads_per_block.y) + 1` calculates how many blocks are needed along the y-dimension (columns).\n",
    "\n",
    "### Last Two Lines of Code in Kernel Configuration\n",
    "The last two lines of the initialization segment where `dim3` structures are used are critical for launching the CUDA kernel:\n",
    "\n",
    "```c\n",
    "dim3 threads_per_block (16, 16, 1); // A 16 x 16 block of threads\n",
    "dim3 number_of_blocks ((N / threads_per_block.x) + 1, (N / threads_per_block.y) + 1, 1);\n",
    "```\n",
    "\n",
    "- `dim3 threads_per_block (16, 16, 1);` — Declares each block to have 16 threads in both the x and y directions. This setting is typical for matrix operations as it nicely maps to the structure of the data.\n",
    "- `dim3 number_of_blocks ((N / threads_per_block.x) + 1, (N / threads_per_block.y) + 1, 1);` — Calculates how many blocks are required to ensure all parts of the matrix are processed. By dividing the total size `N` by the number of threads per dimension and adding one (if necessary), it ensures complete coverage of the matrix.\n",
    "\n",
    "These settings are crucial for optimizing the performance and ensuring that the GPU kernel can handle matrices of size `N x N` efficiently. Each thread computes one element of the result matrix, so the configuration of threads and blocks directly affects the performance and correctness of the matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1ea87-bfb4-484a-9572-8017f06c261d",
   "metadata": {},
   "source": [
    "```\n",
    "__global__ void matrixMulGPU( int * a, int * b, int * c )\n",
    "{\n",
    "  int val = 0;\n",
    "\n",
    "  int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "  if (row < N && col < N)\n",
    "  {\n",
    "    for ( int k = 0; k < N; ++k )\n",
    "      val += a[row * N + k] * b[k * N + col];\n",
    "    c[row * N + col] = val;\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "matrixMulGPU <<< number_of_blocks, threads_per_block >>> ( a, b, c_gpu );\n",
    "\n",
    "\n",
    "explain in detail whats going on. maybe try and explain with an example for ease of visualisation. i cant imagine how are different threads working on a 1D array, which is technically representing a 2D array?\n",
    "\n",
    "---\n",
    "\n",
    "Let's break down how the `matrixMulGPU` kernel function operates, particularly focusing on how it handles a 2D matrix multiplication using a 1D array representation and how threads in a CUDA grid are assigned specific elements to compute. To make this clearer, I'll also include a visual example.\n",
    "\n",
    "### CUDA Matrix Multiplication Explanation\n",
    "\n",
    "1. **Representation of 2D Arrays as 1D**:\n",
    "   - CUDA typically uses 1D arrays to represent 2D matrices in memory. An element at position `(row, col)` in a 2D matrix of size `N x N` is accessed in the 1D array as `index = row * N + col`.\n",
    "   - For example, in a matrix `A` of size `4x4`, the element at `(2, 3)` (3rd row, 4th column) is accessed in a 1D array as `A[2*4 + 3] = A[11]`.\n",
    "\n",
    "2. **Kernel Configuration**:\n",
    "   - The kernel is launched with a configuration of blocks and threads (`number_of_blocks`, `threads_per_block`) such that each thread is responsible for calculating one element of the resulting matrix.\n",
    "   - `dim3 threads_per_block (16, 16)` — This means each block has `256` threads, arranged in a `16x16` grid.\n",
    "   - `dim3 number_of_blocks` — This covers the entire dimension of the matrix `N x N`. For example, if `N=64`, you'd configure it to ensure all elements are covered, considering the block size.\n",
    "\n",
    "3. **Thread Indexing**:\n",
    "   - Each thread in a block has a unique `threadIdx.x` and `threadIdx.y`, and each block has a unique `blockIdx.x` and `blockIdx.y`.\n",
    "   - A thread's absolute position in the matrix is calculated as:\n",
    "     - `int row = blockIdx.x * blockDim.x + threadIdx.x;`\n",
    "     - `int col = blockIdx.y * blockDim.y + threadIdx.y;`\n",
    "   - This calculation maps the thread to a specific element in the matrix. For instance, the first thread in the first block (top-left corner) might compute the first element, while the first thread in the second block might compute the seventeenth element in the first row if each block covers `16` elements horizontally.\n",
    "\n",
    "4. **Matrix Multiplication Operation**:\n",
    "   - For each pair `(row, col)`, the kernel computes the dot product of the `row` of matrix `A` and the `col` of matrix `B`, storing the result in matrix `C`.\n",
    "   - The multiplication and addition for each element are:\n",
    "     - `for (int k = 0; k < N; ++k) { val += a[row * N + k] * b[k * N + col]; }`\n",
    "   - This loops over the entire length `N`, multiplying and accumulating the results to get the final value for `c[row * N + col]`.\n",
    "\n",
    "### Visual Example:\n",
    "Let's consider a small matrix multiplication where `N=4`, and each block has `2x2` threads for simplicity.\n",
    "\n",
    "Suppose you have two matrices:\n",
    "\\[ A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\]\n",
    "\n",
    "In a 1D array, these matrices would be:\n",
    "\\[ A = [1, 2, 3, 4], B = [5, 6, 7, 8] \\]\n",
    "\n",
    "The configuration might be:\n",
    "- `threads_per_block = (2, 2)`\n",
    "- `number_of_blocks = (2, 2)` (for this small example)\n",
    "\n",
    "Threads are mapped as follows:\n",
    "- `Thread(0,0)` in `Block(0,0)` computes `C[0]`\n",
    "- `Thread(1,0)` in `Block(0,0)` computes `C[1]`\n",
    "- `Thread(0,1)` in `Block(0,1)` computes `C[2]`\n",
    "- `Thread(1,1)` in `Block(0,1)` computes `C[3]`\n",
    "\n",
    "Each thread computes:\n",
    "- `C[0]` by `Thread(0,0)` will be `1*5 + 2*7 = 19`\n",
    "- ... and similarly for other elements.\n",
    "\n",
    "By structuring the threads and blocks this way, CUDA enables highly parallel computation of matrix multiplications, taking advantage of the GPU's ability to execute many threads concurrently. This greatly speeds up operations compared to sequential CPU computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4228af1-cbc4-4b95-b010-c54512b162c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
